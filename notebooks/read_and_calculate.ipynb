{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8059600-456c-4a9b-8bb6-24ee24de7026",
   "metadata": {},
   "source": [
    "Notebook with functions for reading data, universal averaging functions, and computing basin means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b12087-45d7-45f0-8b45-9dc5ba293b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# modules for using datetime variables\n",
    "import datetime\n",
    "from datetime import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cftime\n",
    "from pandas.errors import OutOfBoundsDatetime  # Import the specific error\n",
    "\n",
    "from xclim import ensembles\n",
    "import cmip_basins\n",
    "import momlevel as ml # Use Wright EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63e98a-b0bf-4ea5-bd05-553b1a2aca20",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3513257-125a-4b8d-b029-6cdcb32e57a1",
   "metadata": {},
   "source": [
    "## Read netcdf files and get averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd932e6-bead-400a-804b-ac363f9ab0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_custom_decode(data_path, debug=False, dask_chunk=None):\n",
    "    \"\"\"\n",
    "    Reads a dataset and ensures time coordinates are compatible with pre-modern dates using cftime.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the NetCDF file.\n",
    "    - debug (bool): If True, prints debug information.\n",
    "\n",
    "    Returns:\n",
    "    - xr.Dataset: Dataset with time converted to cftime-compatible format.\n",
    "    \"\"\"\n",
    "\n",
    "    if debug:\n",
    "        print(\"START read_data_custom_decode\")\n",
    "    \n",
    "    # Open the dataset without decoding times\n",
    "    if dask_chunk != None:\n",
    "        dataset = xr.open_dataset(data_path, decode_times=False, chunks={'time':dask_chunk})\n",
    "    else:\n",
    "        dataset = xr.open_dataset(data_path, decode_times=False, chunks={'time':1})\n",
    "    if 'time_bnds' in dataset:\n",
    "        dataset = dataset.drop_vars('time_bnds')\n",
    "\n",
    "    if debug:\n",
    "        print(\"From read_data_custom_decode: Opened dataset\")\n",
    "    \n",
    "    # if debug:\n",
    "    #     print(f\"Initial dataset['time'].dtype: {dataset['time'].dtype}\")\n",
    "    #     print(f\"Initial dataset['time'].attrs: {dataset['time'].attrs}\")\n",
    "    \n",
    "    # Extract time attributes\n",
    "    time_units = dataset[\"time\"].attrs.get(\"units\", \"days since 1850-01-01\")\n",
    "    calendar = dataset[\"time\"].attrs.get(\"calendar\", \"gregorian\")\n",
    "    \n",
    "    # Validate time units\n",
    "    if \"since\" not in time_units:\n",
    "        time_units = \"days since 1850-01-01\"  # Fallback\n",
    "    \n",
    "    # Convert time using cftime for full compatibility\n",
    "    try:\n",
    "        times = cftime.num2date(dataset[\"time\"].values, units=time_units, calendar=calendar)\n",
    "        dataset[\"time\"] = xr.DataArray(times, dims=\"time\", name=\"time\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error converting time with cftime: {e}\")\n",
    "        raise\n",
    "\n",
    "    # if debug:\n",
    "    #     print(f\"Final dataset['time'].dtype: {dataset['time'].dtype}\")\n",
    "    #     print(f\"Final dataset['time'].attrs: {dataset['time'].attrs}\")\n",
    "\n",
    "    if debug:\n",
    "        print(\"EXIT read_data_custom_decode\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed9e146-7df3-4be0-9c27-d7fba6d682b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pp_av_data(exp_name,start_yr,end_yr,chunk_length,pp_type='av-annual',diag_file='ocean_monthly_z',time_decoding=True,var=None,month=None,debug=False):\n",
    "    \"\"\"\n",
    "    Getting post-processed data from the production runs.\n",
    "        Args:\n",
    "            exp_name (str)\n",
    "            start_yr (int)\n",
    "            end_yr (int)\n",
    "            chunk_length (int): number of years for av/ts period\n",
    "            pp_type (str): 'av-annual', 'ts-annual', 'av-monthly', or 'ts-monthly'\n",
    "            diag_file (str): pp directory name, such as 'ocean_monthly_z', 'ocean_monthly_rho2', etc.\n",
    "            time_decoding (bool): if True, use xr.open_dataset() with use_cftime=True, otherwise use read_data_custom_decode()\n",
    "            var (str list): required for reading 'ts' files\n",
    "            month (int): value between 1 and 12\n",
    "            debug (bool): if true, give verbose output\n",
    "        Returns:\n",
    "            dataset (xarray dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    # use the control for the static file every time, because sometimes static files don't get saved properly\n",
    "    static_path = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/tune_ctrl_const_200yr/gfdl.ncrc5-intel23-prod/pp/{diag_file}/{diag_file}.static.nc\"\n",
    "    static_ds = xr.open_dataset(static_path)\n",
    "    if debug:\n",
    "        print(f\"Done reading static file.\")\n",
    "    \n",
    "    # static_path = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/{diag_file}.static.nc\"\n",
    "    # static_ds = xr.open_dataset(static_path)\n",
    "    # if debug:\n",
    "    #     print(f\"Done reading static file.\")\n",
    "\n",
    "    # not automatically assigning basin codes anymore\n",
    "    # if \"ocean\" in diag_file:\n",
    "    #     basin_file = xr.open_dataset(\"/archive/Kiera.Lowman/basin_AM2_LM3_MOM6i_1deg.nc\")\n",
    "    #     basin_file = basin_file.rename({'XH': 'xh'})\n",
    "    #     basin_file = basin_file.rename({'YH': 'yh'})\n",
    "    #     basin_file = basin_file.assign_coords({'xh': static_ds['xh'], 'yh': static_ds['yh']})\n",
    "\n",
    "    current_yr = start_yr\n",
    "    final_start_yr = end_yr - chunk_length + 1\n",
    "    if debug:\n",
    "        print(f\"Initial and final start years: {current_yr} and {final_start_yr}\")\n",
    "\n",
    "    ### annual averages ###\n",
    "    if pp_type == 'av-annual':\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/av/annual_{chunk_length}yr\"\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.ann.nc\"\n",
    "\n",
    "        # I should remove dask time chunking for the av-annual and av-monthly data\n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.ann.nc\"\n",
    "                \n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "                \n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    ### annual time series for one or more variables ###\n",
    "    if pp_type == 'ts-annual':\n",
    "        if var is None:\n",
    "            raise IOError(\"'var' list not specified for ts-annual data.\")\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/ts/annual/{chunk_length}yr\"\n",
    "\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[0]}.nc\"\n",
    "        \n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':chunk_length},drop_variables='time_bnds')\n",
    "            # dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds')\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug,dask_chunk=chunk_length)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[0]}.\")\n",
    "            \n",
    "        if len(var) > 1:\n",
    "            for i in range(1,len(var)):\n",
    "                next_var_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[i]}.nc\"\n",
    "                \n",
    "                if time_decoding==True:\n",
    "                    next_var_data = xr.open_dataset(next_var_path,use_cftime=True,chunks={'time':chunk_length},drop_variables='time_bnds')\n",
    "                    # next_var_data = xr.open_dataset(next_var_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds')\n",
    "                else:\n",
    "                    next_var_data = read_data_custom_decode(next_var_path,debug=debug,dask_chunk=chunk_length)\n",
    "                if debug:\n",
    "                    print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[i]}.\")\n",
    "                    \n",
    "                dataset = xr.merge([dataset, next_var_data], compat=\"equals\")\n",
    "                \n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for all variables.\")\n",
    "        \n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            \n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[0]}.nc\"\n",
    "                \n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':chunk_length},drop_variables='time_bnds')\n",
    "                # chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds')\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug,dask_chunk=chunk_length)\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[0]}.\")\n",
    "                \n",
    "            if len(var) > 1:\n",
    "                for i in range(1,len(var)):\n",
    "                    next_var_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[i]}.nc\"\n",
    "                    \n",
    "                    if time_decoding==True:\n",
    "                        next_var_chunk = xr.open_dataset(next_var_path,use_cftime=True,chunks={'time':chunk_length},drop_variables='time_bnds')\n",
    "                        # next_var_chunk = xr.open_dataset(next_var_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds')\n",
    "                    else:\n",
    "                        next_var_chunk = read_data_custom_decode(next_var_path,debug=debug,dask_chunk=chunk_length)\n",
    "                    if debug:\n",
    "                        print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[i]}.\")\n",
    "                        \n",
    "                    chunk_data = xr.merge([chunk_data, next_var_chunk], compat=\"equals\")\n",
    "\n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for all variables.\")\n",
    "                \n",
    "    ### monthly averages for specific month ###\n",
    "    elif pp_type == 'av-monthly' and month != None:\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/av/monthly_{chunk_length}yr\"\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "        \n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    ### monthly averages for all months ###\n",
    "    elif pp_type == 'av-monthly' and month == None:\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/av/monthly_{chunk_length}yr\"\n",
    "        while current_yr <= final_start_yr:\n",
    "            for month in range(1,13):\n",
    "                if debug:\n",
    "                    print(f\"Reading month #{month}\")\n",
    "                if month == 1 and current_yr == start_yr:\n",
    "                    data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "                    if time_decoding==True:\n",
    "                        dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "                    else:\n",
    "                        dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "                else:\n",
    "                    data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "                    if time_decoding==True:\n",
    "                        chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "                    else:\n",
    "                        chunk_data = read_data_custom_decode(data_path,debug=debug)   \n",
    "                    dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} month {str(month)} data.\")\n",
    "\n",
    "            current_yr = current_yr + chunk_length\n",
    "\n",
    "    ### monthly-averaged time series for a SINGLE variable ###\n",
    "    elif pp_type == 'ts-monthly':\n",
    "        if var is None:\n",
    "            raise IOError(\"'var' not specified for ts-monthly data.\")\n",
    "        elif len(var) > 1:\n",
    "            raise IOError(\"Reading ts-monthly data for multiple variables not supported. Provide a list of length 1.\")\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/ts/monthly/{chunk_length}yr\"\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}01-{str(current_yr+chunk_length-1).zfill(4)}12.{var[0]}.nc\"\n",
    "\n",
    "        # should change chunks\n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}01-{str(current_yr+chunk_length-1).zfill(4)}12.{var[0]}.nc\"\n",
    "\n",
    "            # should change chunks\n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    # I think maybe I shouldn't drop \"geolon_u\",\"geolon_v\",\"geolon_c\",\"geolat_u\",\"geolat_v\",\"geolat_c\", but I think I was before because\n",
    "    # xarray was getting mixed up between the different geolon and geolat variables when searching\n",
    "    vars_to_drop = [\"time_bnds\",\"nv\"]\n",
    "    for elem in vars_to_drop:\n",
    "        if elem in dataset:\n",
    "            dataset = dataset.drop_vars(elem)\n",
    "            \n",
    "    if (\"ocean\" in diag_file and \"scalar\" not in diag_file):\n",
    "        # dataset['basin'] = basin_file['basin']\n",
    "        dataset['dxt'] = static_ds['dxt']\n",
    "        dataset['dyt'] = static_ds['dyt']\n",
    "        dataset['dxCu'] = static_ds['dxCu']\n",
    "        dataset['dyCu'] = static_ds['dyCu']\n",
    "        dataset['dxCv'] = static_ds['dxCv']\n",
    "        dataset['dyCv'] = static_ds['dyCv']\n",
    "        dataset['areacello'] = static_ds['areacello']\n",
    "        dataset['areacello_cu'] = static_ds['areacello_cu']\n",
    "        dataset['areacello_cv'] = static_ds['areacello_cv']\n",
    "        dataset['deptho'] = static_ds['deptho']\n",
    "        dataset['wet'] = static_ds['wet']\n",
    "        dataset['geolon'] = static_ds['geolon']\n",
    "        dataset['geolat'] = static_ds['geolat']\n",
    "        dataset['wet_u'] = static_ds['wet_u']\n",
    "        dataset['geolon_u'] = static_ds['geolon_u']\n",
    "        dataset['geolat_u'] = static_ds['geolat_u']\n",
    "        dataset['wet_v'] = static_ds['wet_v']\n",
    "        dataset['geolon_v'] = static_ds['geolon_v']\n",
    "        dataset['geolat_v'] = static_ds['geolat_v']\n",
    "        dataset['wet_c'] = static_ds['wet_c']\n",
    "        dataset['geolon_c'] = static_ds['geolon_c']\n",
    "        dataset['geolat_c'] = static_ds['geolat_c']\n",
    "        dataset = dataset.assign_coords({'geolon': static_ds['geolon'], 'geolat': static_ds['geolat'],\n",
    "                                         'geolon_u': static_ds['geolon_u'], 'geolat_u': static_ds['geolat_u'],\n",
    "                                         'geolon_v': static_ds['geolon_v'], 'geolat_v': static_ds['geolat_v'],\n",
    "                                         'geolon_c': static_ds['geolon_c'], 'geolat_c': static_ds['geolat_c'],})\n",
    "    elif diag_file == \"ocean_monthly\":\n",
    "        if \"volcello\" in dataset:\n",
    "            dataset = dataset.drop_vars(\"volcello\")\n",
    "            dataset = dataset.drop_vars(\"zl\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354d35a1-69f5-44b3-a3d0-c2c07bf08078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of select_basins function from xoverturning\n",
    "\n",
    "def selecting_basins(\n",
    "    ds,\n",
    "    basin=\"global\",\n",
    "    lon=\"geolon\",\n",
    "    lat=\"geolat\",\n",
    "    mask=\"wet\",\n",
    "    verbose=False,\n",
    "    ):\n",
    "    \n",
    "    \"\"\"generate a mask for selected basin\n",
    "\n",
    "    Args:\n",
    "        ds (xarray.Dataset): dataset contaning model grid\n",
    "        basin (str or list, optional): global/atl-arc/indopac/atl/pac/arc/antarc or list of codes. Defaults to \"global\".\n",
    "        lon (str, optional): name of geographical lon in dataset. Defaults to \"geolon\".\n",
    "        lat (str, optional): name of geographical lat in dataset. Defaults to \"geolat\".\n",
    "        mask (str, optional): name of land/sea mask in dataset. Defaults to \"wet\".\n",
    "        verbose (bool, optional): Verbose output. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        xarray.DataArray: mask for selected basin\n",
    "        # xarray.DataArray: mask for MOC streamfunction\n",
    "    \"\"\"\n",
    "\n",
    "    # read or recalculate basin codes\n",
    "    if \"basin\" in ds:\n",
    "        basincodes = ds[\"basin\"]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"generating basin codes\")\n",
    "        basincodes = cmip_basins.generate_basin_codes(ds, lon=lon, lat=lat, mask=mask)\n",
    "\n",
    "    # expand land sea mask to remove other basins\n",
    "    if isinstance(basin, str):\n",
    "        if basin == \"global\":\n",
    "            maxcode = basincodes.max()\n",
    "            assert not np.isnan(maxcode)\n",
    "            selected_codes = np.arange(1, maxcode + 1)\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"glob-no-marg\":\n",
    "            selected_codes = [1, 2, 3, 4, 5] # getting weird AMOC results with inclusion of Med and marginal seas\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"atl-arc\":\n",
    "            selected_codes = [2, 4, 6, 7, 8, 9]\n",
    "            cond1 = ds[lon] < 20.5 # originally 26.5\n",
    "            cond2 = ds[lon] > -70.5\n",
    "            cond3 = basincodes == 1\n",
    "            maskbin = ds[mask].where((basincodes.isin(selected_codes)) | (cond1 & cond2 & cond3))\n",
    "        elif basin == \"atl-arc-south\":\n",
    "            selected_codes = [1, 2, 4, 6, 7, 8, 9]\n",
    "            maskbin = ds[mask].where((basincodes.isin(selected_codes)))\n",
    "        elif basin == \"atl-arc-no-marg\":\n",
    "            selected_codes = [2, 4] # getting weird AMOC results with inclusion of Med and marginal seas\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"indopac\":\n",
    "            selected_codes = [3, 5, 10, 11]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"atl\":\n",
    "            # selected_codes = [2]\n",
    "            cond1 = ds[lon] < 20.5 # originally 26.5\n",
    "            cond2 = ds[lon] > -70.5\n",
    "            cond3 = basincodes == 1\n",
    "            maskbin = ds[mask].where((basincodes == 2) | (cond1 & cond2 & cond3))\n",
    "        elif basin == \"pac\":\n",
    "            # selected_codes = [3]\n",
    "            cond1 = ds[lon] <= -70.5\n",
    "            cond2 = ds[lon] > -210.5\n",
    "            cond3 = basincodes == 1\n",
    "            maskbin = ds[mask].where((basincodes == 3) | (cond1 & cond2 & cond3))\n",
    "        elif basin == \"ind\":\n",
    "            selected_codes = [5]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"arc\":\n",
    "            selected_codes = [4]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"antarc\":\n",
    "            selected_codes = [1]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown basin\")\n",
    "    elif isinstance(basin, list):\n",
    "        for b in basin:\n",
    "            assert isinstance(b, int)\n",
    "        selected_codes = basin\n",
    "    else:\n",
    "        raise ValueError(\"basin must be a string or list of int\")\n",
    "\n",
    "    maskbasin = xr.where(maskbin == 1, True, False)\n",
    "\n",
    "    return maskbasin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5236fdc-2d66-479e-bb5c-a354009c71bb",
   "metadata": {},
   "source": [
    "## Universal computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a809e0a-e0c8-4e40-8db0-69bccdee4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wet and dxt is only defined at the sea surface -- this is ultimately what's causing problems\n",
    "def zonal_mean_v1(da, metrics):\n",
    "    num = (da * metrics['dxt'] * metrics['wet']).sum(dim=['xh'], skipna=True)\n",
    "    denom = (metrics['dxt'] * metrics['wet']).sum(dim=['xh'], skipna=True)\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2675440-61a9-44d4-b2ff-efe68abc4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zonal_mean(da, metrics):\n",
    "#     dxt_3d = xr.where(da.isnull(), np.nan, metrics['dxt'])\n",
    "#     num = (da * dxt_3d).sum(dim=['xh'], skipna=True)\n",
    "#     denom = dxt_3d.sum(dim=['xh'], skipna=True)\n",
    "#     return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec3514b0-a63a-45e7-966b-b5553bb0e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_mean(da, metrics, grid_type='tracer_pt'):\n",
    "    if grid_type=='u_pt':\n",
    "        x_dim = 'xq'\n",
    "        dx_metric = 'dxCu'\n",
    "    elif grid_type=='v_pt':\n",
    "        x_dim = 'xh'\n",
    "        dx_metric = 'dxCv'\n",
    "    else:\n",
    "        x_dim = 'xh'\n",
    "        dx_metric = 'dxt'\n",
    "        \n",
    "    dxt_3d = xr.where(da.isnull(), np.nan, metrics[dx_metric])\n",
    "    num = (da * dxt_3d).sum(dim=[x_dim], skipna=True)\n",
    "    denom = dxt_3d.sum(dim=[x_dim], skipna=True)\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9108aea8-fa6a-4f25-9c6c-89816756d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wet, areacello is only defined at the sea surface -- this is ultimately what's causing problems\n",
    "def horizontal_mean_v1(da, metrics):\n",
    "    num = (da * metrics['areacello'] * metrics['wet']).sum(dim=['xh', 'yh'])\n",
    "    denom = (metrics['areacello'] * metrics['wet']).sum(dim=['xh', 'yh'])\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d967df90-414b-4dcb-adcc-bd49a4cf1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def horizontal_mean(da, metrics):\n",
    "#     area_3d = xr.where(da.isnull(), np.nan, metrics['areacello'])\n",
    "#     num = (da * area_3d).sum(dim=['xh', 'yh'], skipna=True)\n",
    "#     denom = area_3d.sum(dim=['xh', 'yh'], skipna=True)\n",
    "#     return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b320a786-3dc4-428d-9aa9-75feb74f4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_mean(da, metrics, grid_type='tracer_pt'):\n",
    "    if grid_type=='u_pt':\n",
    "        x_dim = 'xq'\n",
    "        y_dim = 'yh'\n",
    "        area_metric = 'areacello_cu'\n",
    "    elif grid_type=='v_pt':\n",
    "        x_dim = 'xh'\n",
    "        y_dim = 'yq'\n",
    "        area_metric = 'areacello_cv'\n",
    "    else:\n",
    "        x_dim = 'xh'\n",
    "        y_dim = 'yh'\n",
    "        area_metric = 'areacello'\n",
    "        \n",
    "    area_3d = xr.where(da.isnull(), np.nan, metrics[area_metric])\n",
    "    num = (da * area_3d).sum(dim=[x_dim, y_dim], skipna=True)\n",
    "    denom = area_3d.sum(dim=[x_dim, y_dim], skipna=True)\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "520ac6af-e4ed-4d35-bc89-25c4be112cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2D_yearly_avg(ds,var,long_1,long_2,lat_1,lat_2):\n",
    "    ds_region = ds.sel(xh=slice(long_1,long_2),yh=slice(lat_1,lat_2))\n",
    "    da_avg = horizontal_mean(ds_region[var], ds_region)\n",
    "    da_avg = da_avg.groupby(da_avg['time'].dt.year).mean(dim='time')\n",
    "    return da_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6bc2c10-e140-4752-b13c-8a3fa5866c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zrho_dat(static_rho,ds_rho,cent_out='cent',x_mean=True,ds_z=None):\n",
    "    \"\"\"\n",
    "    Function for computing depth field from density space data to plot cross-sectional data.\n",
    "\n",
    "    To plot a density-space field as a function of depth, such as the overturning streamfunction, add depth_field\n",
    "    as a coordinate:\n",
    "    # psi.coords['depth'] = depth_field\n",
    "\n",
    "    Returns:\n",
    "        depth_field: depth as a function of zl and y dimension of choice (as determined by cent_out parameter)\n",
    "        \n",
    "    \"\"\"\n",
    "    # note that the vertical coordinate of ds_rho is 'zl' and of ds_z is 'z_l'\n",
    "    \n",
    "    # create a grid using xgcm \n",
    "    coords = {\n",
    "        'X': {'center': 'xh', 'outer': 'xq'},\n",
    "        'Y': {'center': 'yh', 'outer': 'yq'},\n",
    "    }   \n",
    "    metrics = {\n",
    "        'X': [\"dxt\", \"dxCu\", \"dxCv\"],\n",
    "        'Y': [\"dyt\", \"dyCu\", \"dyCv\"]\n",
    "    }\n",
    "    grid = Grid(static_rho, coords=coords, metrics=metrics, periodic=['X'])\n",
    "\n",
    "    # calculate cell thickness from volcello and areacello (I don't have thkcello saved)\n",
    "    if time in ds_rho.dims:\n",
    "        thk  = ds_rho['volcello'].mean(dim='time')/static_rho['areacello']\n",
    "    else:\n",
    "        thk  = ds_rho['volcello']/static_rho['areacello']\n",
    "    thk  = thk.rename('thkcello')\n",
    "\n",
    "    # calculate z from the cell thickness in density space\n",
    "    if x_mean is True:\n",
    "        zrho = thk.mean(dim='xh').cumsum(dim='zl')\n",
    "    else:\n",
    "        # zrho will also be a function of xh\n",
    "        zrho = thk.cumsum(dim='zl')\n",
    "        \n",
    "    zrho = zrho.rename('zrho')\n",
    "\n",
    "    if cent_out == 'cent':\n",
    "        depth_field = zrho\n",
    "    elif cent_out == 'out':\n",
    "        # create depth field defined as a function of yq and potential density (zl)\n",
    "        depth_field = grid.interp(zrho, 'Y', boundary='extend')\n",
    "\n",
    "    # # calculating the average depth\n",
    "    # toz    = ds_z['temp'].mean(dim='time') \n",
    "    # soz    = ds_z['salt'].mean(dim='time') \n",
    "    # sigmaz = ml.derived.calc_pdens(toz, soz, level=2000.0) - 1000.0\n",
    "\n",
    "    # mask   = toz/toz\n",
    "    # delz   = xr.DataArray(np.diff(ds_z['z_i']), dims='z_l')\n",
    "    # dvol   = delz * mask * static_rho['areacello'] \n",
    "    # sigmaz_dvol = sigmaz * dvol\n",
    "    # sigmaz_xave = sigmaz_dvol.sum(dim='xh')/dvol.sum(dim='xh')\n",
    "\n",
    "    return depth_field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17398860-31f2-4c8c-a625-d2f5583576d1",
   "metadata": {},
   "source": [
    "## Some older functions that may or may not be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e74c83b-5884-4c7b-b2ee-1b58b2fcb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averages data by month (i.e. averages all January data)\n",
    "def get_monthly_avg(ds):\n",
    "    ds_avg = ds.groupby(ds['time'].dt.month).mean(dim='time')\n",
    "    ds_avg = ds_avg.assign_coords({'geolon': ds['geolon'].isel(time=0), 'geolat': ds['geolat'].isel(time=0)})\n",
    "    return ds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a3bd3f-065f-4df6-bbb6-ce415374b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporally averages all data\n",
    "def get_time_avg(dataset):\n",
    "    ds_avg = dataset.mean(dim='time')\n",
    "    ds_avg = ds_avg.assign_coords({'geolon': dataset['geolon'].isel(time=0), 'geolat': dataset['geolat'].isel(time=0)})\n",
    "    return ds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7397122-2822-4b9d-b33b-9c8d22b54c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dat_raw(ds1_raw,ds2_raw,var):\n",
    "    da1_raw = ds1_raw[var]\n",
    "    da2_raw = ds2_raw[var]\n",
    "    diff_da = da2_raw - da1_raw\n",
    "    # diff_da_avg = diff_da.mean(dim='time')\n",
    "    diff_da = diff_da.assign_coords({'geolon': da1_raw['geolon'], 'geolat': da1_raw['geolat']})\n",
    "    return diff_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b45351-4be6-494e-80e4-223d7d90c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dat_time_avg(ds1_raw,ds2_raw,var):\n",
    "    da1_raw = ds1_raw[var]\n",
    "    da2_raw = ds2_raw[var]\n",
    "    diff_da = da2_raw - da1_raw\n",
    "    diff_da_avg = diff_da.mean(dim='time')\n",
    "    diff_da_avg = diff_da_avg.assign_coords({'geolon': da1_raw['geolon'].isel(time=0), 'geolat': da1_raw['geolat'].isel(time=0)})\n",
    "    return diff_da_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dfadacc-a34e-4a84-8855-bf2d7d79a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dat_monthly_avg(ds1_raw,ds2_raw,var):\n",
    "    da1_raw = ds1_raw[var]\n",
    "    da2_raw = ds2_raw[var]\n",
    "    diff_da = da2_raw - da1_raw\n",
    "    diff_da_monthly_avg = diff_da.groupby(diff_da['time'].dt.month).mean(dim='time')\n",
    "    diff_da_monthly_avg = diff_da_monthly_avg.assign_coords({'geolon': da1_raw['geolon'].mean(dim='time'), 'geolat': da1_raw['geolat'].mean(dim='time')})\n",
    "    return diff_da_monthly_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a665cee-9e13-4425-a5fa-8c5d23ffccbd",
   "metadata": {},
   "source": [
    "## Basin and cross-section functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8999f649-7fd7-4dd9-b986-0dc061832121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pp_basin_dat(run_dat,var,basin_name,check_nn=False,nn_threshold=0.05,\\\n",
    "                     full_field_var=None,mask_ds=None,verbose=False):\n",
    "    \n",
    "    if mask_ds is None:\n",
    "        masking_dat = run_dat\n",
    "        if verbose:\n",
    "            print(\"mask_ds is none \")\n",
    "    else:\n",
    "        masking_dat = mask_ds\n",
    "\n",
    "    if var == 'u':\n",
    "        lon_metric = 'geolon_u'\n",
    "        lat_metric = 'geolat_u'\n",
    "        mask_metric = 'wet_u'\n",
    "    elif var == 'v':\n",
    "        lon_metric = 'geolon_v'\n",
    "        lat_metric = 'geolat_v'\n",
    "        mask_metric = 'wet_v'\n",
    "    else:\n",
    "        lon_metric = 'geolon'\n",
    "        lat_metric = 'geolat'\n",
    "        mask_metric = 'wet'\n",
    "        \n",
    "    maskbasin = selecting_basins(masking_dat, basin=basin_name, lon=lon_metric, lat=lat_metric, mask=mask_metric, verbose=False)\n",
    "    ds_basin = run_dat.where(maskbasin)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Before masking: \", run_dat[var].dims, run_dat[var].shape)\n",
    "\n",
    "    dat_slice = ds_basin[var]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"After masking, dat_slice: \", dat_slice.dims, dat_slice.shape)\n",
    "        print(f\"Raw basin DATA min and max: {np.nanmin(dat_slice.values)}, \\t {np.nanmax(dat_slice.values)}\")\n",
    "\n",
    "    if mask_ds is None:\n",
    "        if var == 'u':\n",
    "            dat_basin_avg = zonal_mean(dat_slice, ds_basin, grid_type='u_pt')\n",
    "            correct_lat = zonal_mean(run_dat['geolat_u'], run_dat, grid_type='u_pt')\n",
    "        elif var == 'v':\n",
    "            dat_basin_avg = zonal_mean(dat_slice, ds_basin, grid_type='v_pt')\n",
    "            correct_lat = zonal_mean(run_dat['geolat_v'], run_dat, grid_type='v_pt')\n",
    "        else:\n",
    "            dat_basin_avg = zonal_mean(dat_slice, ds_basin)\n",
    "            correct_lat = zonal_mean(run_dat['geolat'], run_dat)\n",
    "    else:\n",
    "        mask_dat_slice = mask_ds[['dxt','wet']].where(maskbasin)\n",
    "        if var == 'u':\n",
    "            dat_basin_avg = zonal_mean(dat_slice, mask_dat_slice, grid_type='u_pt')\n",
    "            correct_lat = zonal_mean(mask_ds['geolat_u'], mask_ds, grid_type='u_pt')\n",
    "        elif var == 'v':\n",
    "            dat_basin_avg = zonal_mean(dat_slice, mask_dat_slice, grid_type='v_pt')\n",
    "            correct_lat = zonal_mean(mask_ds['geolat_v'], mask_ds, grid_type='v_pt')\n",
    "        else:\n",
    "            dat_basin_avg = zonal_mean(dat_slice, mask_dat_slice)\n",
    "            correct_lat = zonal_mean(mask_ds['geolat'], mask_ds)\n",
    "\n",
    "    # print(\"dat_basin_avg.dims\\n:\", dat_basin_avg.dims)\n",
    "    # print(\"dat_basin_avg\\n:\", dat_basin_avg)\n",
    "    # print(\"correct_lat:\\n\", correct_lat)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Basin MEAN min and max: {np.nanmin(dat_basin_avg.values)}, \\t {np.nanmax(dat_basin_avg.values)}\")\n",
    "\n",
    "    if var == 'v':\n",
    "        dat_basin_avg = dat_basin_avg.rename({'yq': 'true_lat'})\n",
    "        dat_basin_avg = dat_basin_avg.assign_coords({'true_lat': correct_lat.values})\n",
    "    else:\n",
    "        dat_basin_avg = dat_basin_avg.rename({'yh': 'true_lat'})\n",
    "        dat_basin_avg = dat_basin_avg.assign_coords({'true_lat': correct_lat.values})\n",
    "    # dat_basin_avg = dat_basin_avg.sortby('true_lat')\n",
    "\n",
    "    # I think this check_nn method is likely checking based on the global mean not null count -- the masking step is setting values equal to null\n",
    "    # everywhere outside of the basin, not changing the size of the dataset\n",
    "    if check_nn:\n",
    "        if full_field_var == None:\n",
    "            not_null = dat_slice.notnull()\n",
    "            if verbose:\n",
    "                print(f\"dat_slice.sizes['xh'] = {dat_slice.sizes['xh']}\")\n",
    "            nn_min = int(dat_slice.sizes['xh']*nn_threshold)\n",
    "        else:\n",
    "            ff_dat_slice = ds_basin[full_field_var]\n",
    "            not_null = ff_dat_slice.notnull()\n",
    "            nn_min = int(ff_dat_slice.sizes['xh']*nn_threshold)\n",
    "            \n",
    "        not_null_int = not_null.astype('int')\n",
    "        not_null_count = not_null_int.sum(dim=['xh'])\n",
    "        not_null_count = not_null_count.rename({'yh': 'true_lat'})\n",
    "        not_null_count['true_lat'] = correct_lat.values\n",
    "        not_null_count = not_null_count.sortby('true_lat')\n",
    "    \n",
    "        # nn_min = int(dat_slice.sizes['xh']*nn_threshold)\n",
    "        \n",
    "        dat_basin_avg = dat_basin_avg.where(not_null_count > nn_min).isel(true_lat=slice(0,-1))\n",
    "        # must remove last row of true_lat values, because otherwise get the error \"ValueError: The input coordinate is not sorted in\n",
    "        # increasing order along axis 0. This can lead to unexpected results. Consider calling the `sortby` method on the input DataArray.\n",
    "        \n",
    "    # else:\n",
    "        # dat_basin_avg = dat_basin_avg.isel(true_lat=slice(0,-1))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"FINAL MEAN min and max: {np.nanmin(dat_basin_avg.values)}, \\t {np.nanmax(dat_basin_avg.values)}\")\n",
    "    \n",
    "    return dat_basin_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e569abb4-e501-47b7-a63d-a2923e446799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to try output zonal means in z-space, remapping from rho-space\n",
    "\n",
    "def get_pp_zrho_basin_dat(run_dat,static_ds,cent_out,var_list,basin_name,check_nn=False,nn_threshold=0.05,\\\n",
    "                     full_field_var=None,mask_ds=None,verbose=False):\n",
    "    # mask_ds contains 'dxt' and 'wet', needed if run_dat does not contain\n",
    "    \n",
    "    # if mask_ds is None:\n",
    "    maskbasin = selecting_basins(run_dat, basin=basin_name, verbose=True)\n",
    "    #     if verbose:\n",
    "    #         print(\"mask_ds is none \")\n",
    "    # else:\n",
    "    #     maskbasin = selecting_basins(mask_ds, basin=basin_name, verbose=True)\n",
    "        \n",
    "    ds_basin = run_dat.where(maskbasin)\n",
    "    dat_slice = ds_basin[var_list]\n",
    "\n",
    "    maskbasin_static = selecting_basins(static_ds, basin=basin_name, verbose=True)\n",
    "    static_basin = static_ds.where(maskbasin)\n",
    "\n",
    "    depth_field = calc_zrho_dat(static_basin, dat_slice, cent_out=cent_out, x_mean=False)\n",
    "    # print(\"depth_field: \\n\",depth_field)\n",
    "\n",
    "    target_depth = np.linspace(float(depth_field.min()), float(depth_field.max()), 100)\n",
    "    def interp_profile(temp_profile, depth_profile, target_depth):\n",
    "        # depth_profile and temp_profile are 1D arrays along 'zl' for one (yh, xh) point.\n",
    "        # np.interp requires the x-values (depth_profile) to be increasing. \n",
    "        # Ensure they are monotonic before using.\n",
    "        return np.interp(target_depth, depth_profile, temp_profile)\n",
    "\n",
    "    temp_on_depth = xr.apply_ufunc(\n",
    "        interp_profile,\n",
    "        dat_slice.temp,         # DataArray with dims ['zl', 'yh', 'xh']\n",
    "        depth_field,  # DataArray with dims ['zl', 'yh', 'xh']\n",
    "        target_depth, # 1D numpy array\n",
    "        input_core_dims=[['zl'], ['zl'], []],\n",
    "        output_core_dims=[['depth']],\n",
    "        vectorize=True,\n",
    "    )\n",
    "    print(temp_on_depth)\n",
    "    \n",
    "    dat_slice = dat_slice.assign_coords(depth = depth_field)\n",
    "    # dat_slice = dat_slice.swap_dims({'zl': 'depth'})\n",
    "    print(\"dat_slice: \\n\", dat_slice)\n",
    "\n",
    "    # if mask_ds is None:\n",
    "    dat_basin_avg = xr.Dataset()\n",
    "    for var in var_list:\n",
    "        var_basin_avg = zonal_mean(dat_slice[var], dat_slice)\n",
    "        dat_basin_avg[var] = var_basin_avg\n",
    "    correct_lat = zonal_mean(run_dat['geolat'], run_dat)\n",
    "    mean_depth = zonal_mean(dat_slice['depth'], dat_slice)\n",
    "    print(\"mean_depth: \\n\", mean_depth)\n",
    "        \n",
    "    # else:\n",
    "    #     mask_dat_slice = mask_ds[['dxt','wet']].where(maskbasin)\n",
    "        \n",
    "    #     dat_basin_avg = xr.Dataset()\n",
    "    #     for var in var_list:\n",
    "    #         var_basin_avg = zonal_mean(dat_slice[var], mask_dat_slice)\n",
    "    #         dat_basin_avg[var] = var_basin_avg\n",
    "    #     correct_lat = zonal_mean(mask_ds['geolat'], mask_ds)\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f\"Basin MEAN min and max: {np.nanmin(dat_basin_avg.values)}, \\t {np.nanmax(dat_basin_avg.values)}\")\n",
    "        \n",
    "    dat_basin_avg = dat_basin_avg.rename({'yh': 'true_lat'})\n",
    "    dat_basin_avg = dat_basin_avg.assign_coords({'true_lat': correct_lat.values})\n",
    "    dat_basin_avg = dat_basin_avg.sortby('true_lat')\n",
    "\n",
    "    mean_depth = mean_depth.rename({'yh': 'true_lat'})\n",
    "    mean_depth = mean_depth.assign_coords({'true_lat': correct_lat.values})\n",
    "    mean_depth = mean_depth.sortby('true_lat')\n",
    "\n",
    "    # if check_nn:\n",
    "    #     if full_field_var == None:\n",
    "    #         not_null = dat_slice.notnull()\n",
    "    #         if verbose:\n",
    "    #             print(f\"dat_slice.sizes['xh'] = {dat_slice.sizes['xh']}\")\n",
    "    #         nn_min = int(dat_slice.sizes['xh']*nn_threshold)\n",
    "    #     else:\n",
    "    #         ff_dat_slice = ds_basin[full_field_var]\n",
    "    #         not_null = ff_dat_slice.notnull()\n",
    "    #         nn_min = int(ff_dat_slice.sizes['xh']*nn_threshold)\n",
    "            \n",
    "    #     not_null_int = not_null.astype('int')\n",
    "    #     not_null_count = not_null_int.sum(dim=['xh'])\n",
    "    #     not_null_count = not_null_count.rename({'yh': 'true_lat'})\n",
    "    #     not_null_count['true_lat'] = correct_lat.values\n",
    "    #     not_null_count = not_null_count.sortby('true_lat')\n",
    "    \n",
    "    #     # nn_min = int(dat_slice.sizes['xh']*nn_threshold)\n",
    "        \n",
    "    #     dat_basin_avg = dat_basin_avg.where(not_null_count > nn_min).isel(true_lat=slice(0,-1))\n",
    "    #     # must remove last row of true_lat values, because otherwise get the error \"ValueError: The input coordinate is not sorted in\n",
    "    #     # increasing order along axis 0. This can lead to unexpected results. Consider calling the `sortby` method on the input DataArray.\n",
    "        \n",
    "    # else:\n",
    "    # print(f\"Min and max of true_lat = -1 position: {np.nanmin(dat_basin_avg.isel(true_lat=-1))}\\t{np.nanmax(dat_basin_avg.isel(true_lat=-1))}\")\n",
    "    # print(f\"Min and max of true_lat = 0 position: {np.nanmin(dat_basin_avg.isel(true_lat=0))}\\t{np.nanmax(dat_basin_avg.isel(true_lat=0))}\")\n",
    "    dat_basin_avg = dat_basin_avg.isel(true_lat=slice(0,-1))\n",
    "    mean_depth = mean_depth.isel(true_lat=slice(0,-1))\n",
    "\n",
    "    # if verbose:\n",
    "    #     # print(\"After zonal_mean: \", dat_basin_avg.dims, dat_basin_avg.shape)\n",
    "    #     print(f\"FINAL MEAN min and max: {np.nanmin(dat_basin_avg.values)}, \\t {np.nanmax(dat_basin_avg.values)}\")\n",
    "    \n",
    "    return dat_basin_avg, depth_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1768ab60-2fe9-4d67-a7fe-24f2f6b89b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basin_horiz_avg(ds,var,basin_name):\n",
    "    ds_region = get_pp_basin_dat(ds,basin_name,var,check_nn=False)\n",
    "    da_avg = horizontal_mean(ds_region[var], ds_region)\n",
    "    da_avg = da_avg.groupby(da_avg['time']).mean(dim='time')\n",
    "    return da_avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
