{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8059600-456c-4a9b-8bb6-24ee24de7026",
   "metadata": {},
   "source": [
    "Notebook with functions for reading data, universal averaging functions, and computing basin means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b12087-45d7-45f0-8b45-9dc5ba293b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# modules for plotting datetime data\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.axis import Axis\n",
    "\n",
    "# modules for using datetime variables\n",
    "import datetime\n",
    "from datetime import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from xgcm import Grid\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cmocean\n",
    "\n",
    "import subprocess as sp\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "from xclim import ensembles\n",
    "from xoverturning import calcmoc\n",
    "import cmip_basins\n",
    "\n",
    "import cftime\n",
    "from pandas.errors import OutOfBoundsDatetime  # Import the specific error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63e98a-b0bf-4ea5-bd05-553b1a2aca20",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3513257-125a-4b8d-b029-6cdcb32e57a1",
   "metadata": {},
   "source": [
    "## Read netcdf files and get averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd932e6-bead-400a-804b-ac363f9ab0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_custom_decode(data_path, debug=False):\n",
    "    \"\"\"\n",
    "    Reads a dataset and ensures time coordinates are compatible with pre-modern dates using cftime.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the NetCDF file.\n",
    "    - debug (bool): If True, prints debug information.\n",
    "\n",
    "    Returns:\n",
    "    - xr.Dataset: Dataset with time converted to cftime-compatible format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the dataset without decoding times\n",
    "    dataset = xr.open_dataset(data_path, decode_times=False, chunks={'time':1})\n",
    "    if 'time_bnds' in dataset:\n",
    "        dataset = dataset.drop_vars('time_bnds')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Initial dataset['time'].dtype: {dataset['time'].dtype}\")\n",
    "        print(f\"Initial dataset['time'].attrs: {dataset['time'].attrs}\")\n",
    "    \n",
    "    # Extract time attributes\n",
    "    time_units = dataset[\"time\"].attrs.get(\"units\", \"days since 1850-01-01\")\n",
    "    calendar = dataset[\"time\"].attrs.get(\"calendar\", \"gregorian\")\n",
    "    \n",
    "    # Validate time units\n",
    "    if \"since\" not in time_units:\n",
    "        time_units = \"days since 1850-01-01\"  # Fallback\n",
    "    \n",
    "    # Convert time using cftime for full compatibility\n",
    "    try:\n",
    "        times = cftime.num2date(dataset[\"time\"].values, units=time_units, calendar=calendar)\n",
    "        dataset[\"time\"] = xr.DataArray(times, dims=\"time\", name=\"time\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error converting time with cftime: {e}\")\n",
    "        raise\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Final dataset['time'].dtype: {dataset['time'].dtype}\")\n",
    "        print(f\"Final dataset['time'].attrs: {dataset['time'].attrs}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed9e146-7df3-4be0-9c27-d7fba6d682b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pp_av_data(exp_name,start_yr,end_yr,chunk_length,pp_type='av-annual',diag_file='ocean_monthly_z',time_decoding=True,var=None,month=None,debug=False):\n",
    "    \"\"\"\n",
    "    Getting post-processed data from the production runs.\n",
    "        Args:\n",
    "            exp_name (str)\n",
    "            start_yr (int)\n",
    "            end_yr (int)\n",
    "            chunk_length (int): number of years for av/ts period\n",
    "            pp_type (str): 'av-annual', 'ts-annual', 'av-monthly', or 'ts-monthly'\n",
    "            diag_file (str): pp directory name, such as 'ocean_monthly_z', 'ocean_monthly_rho2', etc.\n",
    "            time_decoding (bool): if True, use xr.open_dataset() with use_cftime=True, otherwise use read_data_custom_decode()\n",
    "            var (str list): required for reading 'ts' files\n",
    "            month (int): value between 1 and 12\n",
    "            debug (bool): if true, give verbose output\n",
    "        Returns:\n",
    "            dataset (xarray dataset)\n",
    "    \"\"\"\n",
    "    \n",
    "    static_path = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/{diag_file}.static.nc\"\n",
    "    static_ds = xr.open_dataset(static_path)\n",
    "    if debug:\n",
    "        print(f\"Done reading static file.\")\n",
    "\n",
    "    static_vars_to_drop = [\"geolon_u\",\"geolon_v\",\"geolon_c\",\"geolat_u\",\"geolat_v\",\"geolat_c\",\"wet_v\"]\n",
    "    for elem in static_vars_to_drop:\n",
    "        if elem in static_ds:\n",
    "            static_ds = static_ds.drop_vars(elem)\n",
    "\n",
    "    if \"ocean\" in diag_file:\n",
    "        basin_file = xr.open_dataset(\"/archive/Kiera.Lowman/basin_AM2_LM3_MOM6i_1deg.nc\")\n",
    "        basin_file = basin_file.rename({'XH': 'xh'})\n",
    "        basin_file = basin_file.rename({'YH': 'yh'})\n",
    "        basin_file = basin_file.assign_coords({'xh': static_ds['xh'], 'yh': static_ds['yh']})\n",
    "\n",
    "    current_yr = start_yr\n",
    "    final_start_yr = end_yr - chunk_length + 1\n",
    "    if debug:\n",
    "        print(f\"Initial and final start years: {current_yr} and {final_start_yr}\")\n",
    "        \n",
    "    # ### annual averages and time series ###\n",
    "    # if (pp_type == 'av-annual' or pp_type == 'ts-annual') and month == None:\n",
    "    #     if pp_type == 'av-annual':\n",
    "    #         path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/av/annual_{chunk_length}yr\"\n",
    "    #         data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.ann.nc\"\n",
    "    #     elif pp_type == 'ts-annual':\n",
    "    #         if var is None:\n",
    "    #             raise IOError(\"'var' not specified for ts-annual data.\")\n",
    "    #         path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/ts/annual/{chunk_length}yr\"\n",
    "    #         data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var}.nc\"\n",
    "\n",
    "    #     if time_decoding==True:\n",
    "    #         dataset = xr.open_dataset(data_path,use_cftime=True)\n",
    "    #     else:\n",
    "    #         dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "    #     if debug:\n",
    "    #         print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    #     while current_yr < final_start_yr:\n",
    "    #         current_yr = current_yr + chunk_length\n",
    "    #         if pp_type == 'av-annual':\n",
    "    #             data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.ann.nc\"\n",
    "    #         elif pp_type == 'ts-annual':\n",
    "    #             data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var}.nc\"\n",
    "                \n",
    "    #         if time_decoding==True:\n",
    "    #             chunk_data = xr.open_dataset(data_path,use_cftime=True)\n",
    "    #         else:\n",
    "    #             chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "                \n",
    "    #         dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "    #         if debug:\n",
    "    #             print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    ### annual averages ###\n",
    "    if pp_type == 'av-annual':\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/av/annual_{chunk_length}yr\"\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.ann.nc\"\n",
    "\n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.ann.nc\"\n",
    "                \n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "                \n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    ### annual time series for one or more variables ###\n",
    "    if pp_type == 'ts-annual':\n",
    "        if var is None:\n",
    "            raise IOError(\"'var' list not specified for ts-annual data.\")\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/ts/annual/{chunk_length}yr\"\n",
    "\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[0]}.nc\"\n",
    "\n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds') #added dask time chunk\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[0]}.\")\n",
    "            \n",
    "        if len(var) > 1:\n",
    "            for i in range(1,len(var)):\n",
    "                next_var_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[i]}.nc\"\n",
    "                \n",
    "                if time_decoding==True:\n",
    "                    next_var_data = xr.open_dataset(next_var_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds')\n",
    "                else:\n",
    "                    next_var_data = read_data_custom_decode(next_var_path,debug=debug)\n",
    "                if debug:\n",
    "                    print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[i]}.\")\n",
    "                    \n",
    "                dataset = xr.merge([dataset, next_var_data], compat=\"equals\")\n",
    "                \n",
    "            if debug:\n",
    "                        print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for all variables.\")\n",
    "        \n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            \n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[0]}.nc\"\n",
    "                \n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds')\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[0]}.\")\n",
    "                \n",
    "            if len(var) > 1:\n",
    "                for i in range(1,len(var)):\n",
    "                    next_var_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{var[i]}.nc\"\n",
    "                    \n",
    "                    if time_decoding==True:\n",
    "                        next_var_chunk = xr.open_dataset(next_var_path,use_cftime=True,chunks={'time':1},drop_variables='time_bnds')\n",
    "                    else:\n",
    "                        next_var_chunk = read_data_custom_decode(next_var_path,debug=debug)\n",
    "                    if debug:\n",
    "                        print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for {var[i]}.\")\n",
    "                        \n",
    "                    chunk_data = xr.merge([chunk_data, next_var_chunk], compat=\"equals\")\n",
    "\n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data for all variables.\")\n",
    "                \n",
    "    ### monthly averages for specific month ###\n",
    "    elif pp_type == 'av-monthly' and month != None:\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/av/monthly_{chunk_length}yr\"\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "        \n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    ### monthly averages for all months ###\n",
    "    elif pp_type == 'av-monthly' and month == None:\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/av/monthly_{chunk_length}yr\"\n",
    "        while current_yr <= final_start_yr:\n",
    "            for month in range(1,13):\n",
    "                if debug:\n",
    "                    print(f\"Reading month #{month}\")\n",
    "                if month == 1 and current_yr == start_yr:\n",
    "                    data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "                    if time_decoding==True:\n",
    "                        dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "                    else:\n",
    "                        dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "                else:\n",
    "                    data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}-{str(current_yr+chunk_length-1).zfill(4)}.{str(month).zfill(2)}.nc\"\n",
    "                    if time_decoding==True:\n",
    "                        chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "                    else:\n",
    "                        chunk_data = read_data_custom_decode(data_path,debug=debug)   \n",
    "                    dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} month {str(month)} data.\")\n",
    "\n",
    "            current_yr = current_yr + chunk_length\n",
    "\n",
    "    ### monthly-averaged time series for a single variable ###\n",
    "    elif pp_type == 'ts-monthly':\n",
    "        if var is None:\n",
    "            raise IOError(\"'var' not specified for ts-monthly data.\")\n",
    "        elif len(var) > 1:\n",
    "            raise IOError(\"Reading ts-monthly data for multiple variables not supported. Provide a list of length 1.\")\n",
    "        path_prefix = f\"/archive/Kiera.Lowman/FMS2019.01.03_devgfdl_20201120_kiera/{exp_name}/gfdl.ncrc5-intel23-prod/pp/{diag_file}/ts/monthly/{chunk_length}yr\"\n",
    "        data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}01-{str(current_yr+chunk_length-1).zfill(4)}12.{var[0]}.nc\"\n",
    "\n",
    "        if time_decoding==True:\n",
    "            dataset = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "        else:\n",
    "            dataset = read_data_custom_decode(data_path,debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "        while current_yr < final_start_yr:\n",
    "            current_yr = current_yr + chunk_length\n",
    "            data_path = f\"{path_prefix}/{diag_file}.{str(current_yr).zfill(4)}01-{str(current_yr+chunk_length-1).zfill(4)}12.{var[0]}.nc\"\n",
    "                \n",
    "            if time_decoding==True:\n",
    "                chunk_data = xr.open_dataset(data_path,use_cftime=True,chunks={'time':1})\n",
    "            else:\n",
    "                chunk_data = read_data_custom_decode(data_path,debug=debug)\n",
    "            dataset = xr.concat([dataset,chunk_data],\"time\")\n",
    "            if debug:\n",
    "                print(f\"Done reading year {current_yr} to {current_yr+chunk_length-1} data.\")\n",
    "\n",
    "    vars_to_drop = [\"time_bnds\",\"nv\",\"geolon_u\",\"geolon_v\",\"geolon_c\",\"geolat_u\",\"geolat_v\",\"geolat_c\"]\n",
    "    for elem in vars_to_drop:\n",
    "        if elem in dataset:\n",
    "            dataset = dataset.drop_vars(elem)\n",
    "            \n",
    "    if \"ocean\" in diag_file and \"scalar\" not in diag_file:\n",
    "        dataset['basin'] = basin_file['basin']\n",
    "        dataset['dxt'] = static_ds['dxt']\n",
    "        dataset['dyt'] = static_ds['dyt']\n",
    "        dataset['areacello'] = static_ds['areacello']\n",
    "        dataset['wet'] = static_ds['wet']\n",
    "        dataset['geolon'] = static_ds['geolon']\n",
    "        dataset['geolat'] = static_ds['geolat']\n",
    "        dataset = dataset.assign_coords({'geolon': static_ds['geolon'], 'geolat': static_ds['geolat']})\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354d35a1-69f5-44b3-a3d0-c2c07bf08078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of select_basins function from xoverturning\n",
    "\n",
    "def selecting_basins(\n",
    "    ds,\n",
    "    basin=\"global\",\n",
    "    lon=\"geolon\",\n",
    "    lat=\"geolat\",\n",
    "    mask=\"wet\",\n",
    "    verbose=True,\n",
    "    ):\n",
    "    \n",
    "    \"\"\"generate a mask for selected basin\n",
    "\n",
    "    Args:\n",
    "        ds (xarray.Dataset): dataset contaning model grid\n",
    "        basin (str or list, optional): global/atl-arc/indopac/atl/pac/arc/antarc or list of codes. Defaults to \"global\".\n",
    "        lon (str, optional): name of geographical lon in dataset. Defaults to \"geolon\".\n",
    "        lat (str, optional): name of geographical lat in dataset. Defaults to \"geolat\".\n",
    "        mask (str, optional): name of land/sea mask in dataset. Defaults to \"wet\".\n",
    "        verbose (bool, optional): Verbose output. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        xarray.DataArray: mask for selected basin\n",
    "        # xarray.DataArray: mask for MOC streamfunction\n",
    "    \"\"\"\n",
    "\n",
    "    # read or recalculate basin codes\n",
    "    if \"basin\" in ds:\n",
    "        basincodes = ds[\"basin\"]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"generating basin codes\")\n",
    "        basincodes = cmip_basins.generate_basin_codes(ds, lon=lon, lat=lat, mask=mask)\n",
    "\n",
    "    # expand land sea mask to remove other basins\n",
    "    if isinstance(basin, str):\n",
    "        if basin == \"global\":\n",
    "            maxcode = basincodes.max()\n",
    "            assert not np.isnan(maxcode)\n",
    "            selected_codes = np.arange(1, maxcode + 1)\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"glob-no-marg\":\n",
    "            selected_codes = [1, 2, 3, 4, 5] # getting weird AMOC results with inclusion of Med and marginal seas\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"atl-arc\":\n",
    "            selected_codes = [2, 4, 6, 7, 8, 9]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"atl-arc-no-marg\":\n",
    "            selected_codes = [2, 4] # getting weird AMOC results with inclusion of Med and marginal seas\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"indopac\":\n",
    "            selected_codes = [3, 5, 10, 11]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"atl\":\n",
    "            selected_codes = [2]\n",
    "            cond1 = ds[lon] < 26.5\n",
    "            cond2 = ds[lon] > -72.5\n",
    "            cond3 = ds[\"basin\"] == 1\n",
    "            maskbin = ds[mask].where((basincodes == 2) | (cond1 & cond2 & cond3))\n",
    "        elif basin == \"pac\":\n",
    "            selected_codes = [3]\n",
    "            cond1 = ds[lon] < -68.5\n",
    "            cond2 = ds[lon] > -210.5\n",
    "            cond3 = ds[\"basin\"] == 1\n",
    "            maskbin = ds[mask].where((basincodes == 3) | (cond1 & cond2 & cond3))\n",
    "        elif basin == \"ind\":\n",
    "            selected_codes = [5]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"arc\":\n",
    "            selected_codes = [4]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        elif basin == \"antarc\":\n",
    "            selected_codes = [1]\n",
    "            maskbin = ds[mask].where(basincodes.isin(selected_codes))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown basin\")\n",
    "    elif isinstance(basin, list):\n",
    "        for b in basin:\n",
    "            assert isinstance(b, int)\n",
    "        selected_codes = basin\n",
    "    else:\n",
    "        raise ValueError(\"basin must be a string or list of int\")\n",
    "\n",
    "    maskbasin = xr.where(maskbin == 1, True, False)\n",
    "\n",
    "    return maskbasin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5236fdc-2d66-479e-bb5c-a354009c71bb",
   "metadata": {},
   "source": [
    "## Universal computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a809e0a-e0c8-4e40-8db0-69bccdee4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_mean(da, metrics):\n",
    "    num = (da * metrics['dxt'] * metrics['wet']).sum(dim=['xh'])\n",
    "    denom = (metrics['dxt'] * metrics['wet']).sum(dim=['xh'])\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9108aea8-fa6a-4f25-9c6c-89816756d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_mean(da, metrics):\n",
    "    num = (da * metrics['areacello'] * metrics['wet']).sum(dim=['xh', 'yh'])\n",
    "    denom = (metrics['areacello'] * metrics['wet']).sum(dim=['xh', 'yh'])\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520ac6af-e4ed-4d35-bc89-25c4be112cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2D_yearly_avg(ds,var,long_1,long_2,lat_1,lat_2):\n",
    "    ds_region = ds.sel(xh=slice(long_1,long_2),yh=slice(lat_1,lat_2))\n",
    "    da_avg = horizontal_mean(ds_region[var], ds_region)\n",
    "    da_avg = da_avg.groupby(da_avg['time'].dt.year).mean(dim='time')\n",
    "    return da_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17398860-31f2-4c8c-a625-d2f5583576d1",
   "metadata": {},
   "source": [
    "## Some older functions that may or may not be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e74c83b-5884-4c7b-b2ee-1b58b2fcb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averages data by month (i.e. averages all January data)\n",
    "def get_monthly_avg(ds):\n",
    "    ds_avg = ds.groupby(ds['time'].dt.month).mean(dim='time')\n",
    "    ds_avg = ds_avg.assign_coords({'geolon': ds['geolon'].isel(time=0), 'geolat': ds['geolat'].isel(time=0)})\n",
    "    return ds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a3bd3f-065f-4df6-bbb6-ce415374b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporally averages all data\n",
    "def get_time_avg(dataset):\n",
    "    ds_avg = dataset.mean(dim='time')\n",
    "    ds_avg = ds_avg.assign_coords({'geolon': dataset['geolon'].isel(time=0), 'geolat': dataset['geolat'].isel(time=0)})\n",
    "    return ds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7397122-2822-4b9d-b33b-9c8d22b54c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dat_raw(ds1_raw,ds2_raw,var):\n",
    "    da1_raw = ds1_raw[var]\n",
    "    da2_raw = ds2_raw[var]\n",
    "    diff_da = da2_raw - da1_raw\n",
    "    # diff_da_avg = diff_da.mean(dim='time')\n",
    "    diff_da = diff_da.assign_coords({'geolon': da1_raw['geolon'], 'geolat': da1_raw['geolat']})\n",
    "    return diff_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b45351-4be6-494e-80e4-223d7d90c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dat_time_avg(ds1_raw,ds2_raw,var):\n",
    "    da1_raw = ds1_raw[var]\n",
    "    da2_raw = ds2_raw[var]\n",
    "    diff_da = da2_raw - da1_raw\n",
    "    diff_da_avg = diff_da.mean(dim='time')\n",
    "    diff_da_avg = diff_da_avg.assign_coords({'geolon': da1_raw['geolon'].isel(time=0), 'geolat': da1_raw['geolat'].isel(time=0)})\n",
    "    return diff_da_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dfadacc-a34e-4a84-8855-bf2d7d79a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dat_monthly_avg(ds1_raw,ds2_raw,var):\n",
    "    da1_raw = ds1_raw[var]\n",
    "    da2_raw = ds2_raw[var]\n",
    "    diff_da = da2_raw - da1_raw\n",
    "    diff_da_monthly_avg = diff_da.groupby(diff_da['time'].dt.month).mean(dim='time')\n",
    "    diff_da_monthly_avg = diff_da_monthly_avg.assign_coords({'geolon': da1_raw['geolon'].mean(dim='time'), 'geolat': da1_raw['geolat'].mean(dim='time')})\n",
    "    return diff_da_monthly_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a665cee-9e13-4425-a5fa-8c5d23ffccbd",
   "metadata": {},
   "source": [
    "## Basin and cross-section functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8999f649-7fd7-4dd9-b986-0dc061832121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pp_basin_dat(run_ds,basin_name,var,check_nn=True,nn_threshold=0.05,full_field_var=None,mask_ds=None,\\\n",
    "                     single_var_da=False,verbose=False):\n",
    "    \n",
    "    if mask_ds is None:\n",
    "        maskbasin = selecting_basins(run_ds, basin=basin_name, verbose=False)\n",
    "        if verbose:\n",
    "            print(\"mask_ds is none \")\n",
    "    else:\n",
    "        maskbasin = selecting_basins(mask_ds, basin=basin_name, verbose=False)\n",
    "        \n",
    "    ds_basin = run_ds.where(maskbasin)\n",
    "\n",
    "    if single_var_da == False:\n",
    "        dat_slice = ds_basin[var]\n",
    "    else:\n",
    "        dat_slice = ds_basin\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Min: {np.nanmin(dat_slice.values)} \\t Max: {np.nanmax(dat_slice.values)}\")\n",
    "\n",
    "    if mask_ds is None:\n",
    "        dat_basin_avg = zonal_mean(dat_slice, ds_basin)\n",
    "        correct_lat = zonal_mean(run_ds['geolat'], run_ds)\n",
    "    else:\n",
    "        mask_dat_slice = mask_ds[['dxt','wet']].where(maskbasin)\n",
    "        dat_basin_avg = zonal_mean(dat_slice, mask_dat_slice)\n",
    "        correct_lat = zonal_mean(mask_ds['geolat'], mask_ds)\n",
    "    \n",
    "    dat_basin_avg = dat_basin_avg.rename({'yh': 'true_lat'})\n",
    "    dat_basin_avg = dat_basin_avg.assign_coords({'true_lat': correct_lat.values})\n",
    "    dat_basin_avg = dat_basin_avg.sortby('true_lat')\n",
    "\n",
    "    if check_nn:\n",
    "        if full_field_var == None:\n",
    "            not_null = dat_slice.notnull()\n",
    "            if verbose:\n",
    "                print(f\"dat_slice.sizes['xh'] = {dat_slice.sizes['xh']}\")\n",
    "            nn_min = int(dat_slice.sizes['xh']*nn_threshold)\n",
    "        else:\n",
    "            ff_dat_slice = ds_basin[full_field_var]\n",
    "            not_null = ff_dat_slice.notnull()\n",
    "            nn_min = int(ff_dat_slice.sizes['xh']*nn_threshold)\n",
    "            \n",
    "        not_null_int = not_null.astype('int')\n",
    "        not_null_count = not_null_int.sum(dim=['xh'])\n",
    "        not_null_count = not_null_count.rename({'yh': 'true_lat'})\n",
    "        not_null_count['true_lat'] = correct_lat.values\n",
    "        not_null_count = not_null_count.sortby('true_lat')\n",
    "    \n",
    "        # nn_min = int(dat_slice.sizes['xh']*nn_threshold)\n",
    "    \n",
    "        dat_basin_avg = dat_basin_avg.where(not_null_count > nn_min).isel(true_lat=slice(0,-1))\n",
    "        \n",
    "    else:\n",
    "        dat_basin_avg = dat_basin_avg.isel(true_lat=slice(0,-1))\n",
    "    \n",
    "    return dat_basin_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1768ab60-2fe9-4d67-a7fe-24f2f6b89b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basin_horiz_avg(ds,var,basin_name):\n",
    "    ds_region = get_pp_basin_dat(ds,basin_name,var,check_nn=False)\n",
    "    da_avg = horizontal_mean(ds_region[var], ds_region)\n",
    "    da_avg = da_avg.groupby(da_avg['time']).mean(dim='time')\n",
    "    return da_avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
